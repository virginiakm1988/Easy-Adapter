{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1VuKPcabUludhy2qeuw2d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virginiakm1988/VGC-adapter/blob/main/Adapters_are_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install package && setup\n"
      ],
      "metadata": {
        "id": "URCCnICCUYbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets\n",
        "! pip install loralib"
      ],
      "metadata": {
        "id": "lynkbdevNmwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEVCf8AdJbbw",
        "outputId": "5dfd6370-fc76-4d43-c682-1e833ee97034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  5 11:20:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define custom adapter modules\n",
        "Here we implemented `Houlsby`, `ConvAdapters`, `AdapterBias`, and `LoRA`.\n",
        "1. Houlsby Adapter ([Parameter-Efficient Transfer Learning for NLP](https://http://proceedings.mlr.press/v97/houlsby19a.html))\n",
        "2. ConvAdapter ([CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models](https://arxiv.org/abs/2212.01282))\n",
        "3. AdapterBias ([AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks](https://arxiv.org/abs/2205.00305))\n",
        "\n",
        "4. LoRA ([LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685))\n",
        "\n",
        "5. BitFit ([BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)\n",
        "```\n",
        "mark_only_adapter_as_trainable(model_bert,bias=\"all\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TVjQ1JTSUfNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "  \n",
        "## Houlsby adapter\n",
        "class Houlsby_Adapter(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            bottleneck = 32\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.houlsby_adapter = nn.Sequential(\n",
        "          nn.Linear(input_size, bottleneck),\n",
        "          nn.GELU(),\n",
        "          nn.Linear(bottleneck, input_size),\n",
        "      )\n",
        "    def forward(self, x):\n",
        "        return self.houlsby_adapter(x)\n",
        "\n",
        "## conv adapter\n",
        "class Conv_Adapter(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            compress_rate = 8,\n",
        "            k = 1, \n",
        "            stride = 1,\n",
        "            dropout = 0.8\n",
        "        ):\n",
        "        super().__init__()\n",
        "        def depthwise_conv(n_in, n_out, compress_rate, k, stride):\n",
        "          conv = nn.Conv1d(n_in, n_out//compress_rate, k, stride = stride)\n",
        "          nn.init.kaiming_normal_(conv.weight)\n",
        "          return conv\n",
        "        def pointwise_conv(n_in, n_out, compress_rate, k, stride):\n",
        "          conv = nn.Conv1d(n_out//compress_rate,n_out, 1)\n",
        "          nn.init.kaiming_normal_(conv.weight)\n",
        "          return conv\n",
        "        self.conv_adapter = nn.Sequential(\n",
        "        depthwise_conv(input_size, input_size, compress_rate,k ,stride),\n",
        "        pointwise_conv(input_size, input_size, compress_rate,k ,stride),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.GELU()\n",
        "      )\n",
        "    def forward(self, x):\n",
        "        return self.conv_adapter(x)\n",
        "\n",
        "## adapterBias\n",
        "class AdapterBias(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            dropout = 0.8\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.adapter_vector = nn.Parameter(torch.ones((input_size), requires_grad=True))\n",
        "        self.adapter_alpha = nn.Linear(input_size, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.adapter_vector  * self.adapter_alpha(x)\n",
        "##lora\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            dropout = 0.8,\n",
        "            r = 16\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.lora_adapter = lora.Linear(input_size, input_size, r)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.lora_adapter(x)"
      ],
      "metadata": {
        "id": "m7HU18BMNitf"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "0BgIUzO1eTwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "def mark_only_adapter_as_trainable(model: nn.Module, bias: str = 'none') -> None:\n",
        "    for n, p in model.named_parameters():\n",
        "        \n",
        "        if 'adapter' not in n:\n",
        "          p.requires_grad = False\n",
        "        else:\n",
        "          p.requires_grad = True\n",
        "    if bias == \"none\":\n",
        "      return\n",
        "    elif bias == 'all':\n",
        "        for n, p in model.named_parameters():\n",
        "            if 'bias' in n:\n",
        "                p.requires_grad = True\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def adapter_state_dict(model: nn.Module, bias: str = 'none') -> Dict[str, torch.Tensor]:\n",
        "    my_state_dict = model.state_dict()\n",
        "    if bias == 'none':\n",
        "        return {k: my_state_dict[k] for k in my_state_dict if 'adapter' in k}\n",
        "    elif bias == 'all':\n",
        "        return {k: my_state_dict[k] for k in my_state_dict if 'adapter_' in k or 'bias' in k}\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "L7Cl73UQdwi4"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding adapter to a hugginface model"
      ],
      "metadata": {
        "id": "tfG0YD5ngHnQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Ec2TeCuA-YrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4325f5-ee0d-404c-db04-2f60935ca14f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "BertLayerNorm = torch.nn.LayerNorm\n",
        "\n",
        "##vanilla houlsby residual adapter, custom layers\n",
        "class adapted_bert_output(nn.Module):\n",
        "  def __init__(self, BertOutput, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    if config.adapter == \"houlsby\":\n",
        "      self.adapter = Houlsby_Adapter(config.hidden_size)\n",
        "    elif config.adapter == \"conv_adapter\":\n",
        "      self.adapter = Conv_Adapter(config.max_position_embeddings)\n",
        "    elif self.adapter == \"AdapterBias\":\n",
        "      self.adapter = AdapterBias(config.hidden_size)\n",
        "    elif self.adapter == \"lora\":\n",
        "      self.adapter = LoRA(config.hidden_size)\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def forward(self,  hidden_states, input_tensor):\n",
        "\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    if self.config.adapter != None:\n",
        "      adapter_output = self.adapter(hidden_states)\n",
        "      hidden_states = self.dropout(hidden_states) + adapter_output\n",
        "    else:\n",
        "      hidden_states = self.dropout(hidden_states)\n",
        "    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "  \n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "model_bert.config.adapter = \"houlsby\"\n",
        "#add adapter module in a bert model\n",
        "for idx, layer in enumerate(model_bert.bert.encoder.layer):\n",
        "  model_bert.bert.encoder.layer[idx].output = adapted_bert_output(model_bert.bert.encoder.layer[idx].output, model_bert.config)\n",
        "\n",
        "#freeze parameters\n",
        "mark_only_adapter_as_trainable(model_bert)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading datasets"
      ],
      "metadata": {
        "id": "KFC1q8YzWfSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"test\"]\n"
      ],
      "metadata": {
        "id": "RQukWUmBWdY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start training"
      ],
      "metadata": {
        "id": "hsyZ19r6WkBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(output_dir = \"test-trainer\",per_device_train_batch_size = 4)\n",
        "trainer = Trainer(\n",
        "    model=model_bert, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
        ")\n",
        "trainer.train() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "i4oPxeZrWebn",
        "outputId": "12327bd9-0924-427f-9ee6-fe149f5b5208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 01:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.706200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=750, training_loss=0.7052169596354166, metrics={'train_runtime': 93.2405, 'train_samples_per_second': 32.175, 'train_steps_per_second': 8.044, 'total_flos': 792051075072000.0, 'train_loss': 0.7052169596354166, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving adapter checkpoints\n"
      ],
      "metadata": {
        "id": "_5vqaOMRgYqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./result\"\n",
        "torch.save(adapter_state_dict(model_bert), checkpoint_path)"
      ],
      "metadata": {
        "id": "kcJ3ygwogbwg"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding LoRA in selfattention"
      ],
      "metadata": {
        "id": "0qah-UB2SCEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import loralib as lora\n",
        "import math\n",
        "class AdaptedBertSelfAttention(nn.Module):\n",
        "  def __init__(self, BertSelfAttention, config):\n",
        "    super().__init__()\n",
        "    self.num_attention_heads = config.num_attention_heads\n",
        "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "    self.query = lora.Linear(config.hidden_size, self.all_head_size, config.lora_r, lora_alpha=config.lora_alpha)\n",
        "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "    self.value = lora.Linear(config.hidden_size, self.all_head_size, config.lora_r, lora_alpha=config.lora_alpha)\n",
        "\n",
        "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "    self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "    if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "    self.is_decoder = config.is_decoder\n",
        "\n",
        "  def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "aJPoFwcTSDXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#add lora adapter module in a bert model by replacing the selfattention function\n",
        "model_bert.config.lora_r = 8\n",
        "model_bert.config.lora_alpha = 8\n",
        "for idx, layer in enumerate(model_bert.bert.encoder.layer):\n",
        "  model_bert.bert.encoder.layer[idx].attention.self = AdaptedBertSelfAttention(model_bert.bert.encoder.layer[idx].attention.self, model_bert.config)\n",
        "for name, param in model_bert.named_parameters():\n",
        "  if \"lora\" in name:\n",
        "    param.require_grad = True\n",
        "  else:\n",
        "    param.require_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pafce7U0S5gp",
        "outputId": "69f23600-12d9-48b2-93ea-edd0cea5cfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\"test_trainer\")\n",
        "trainer = Trainer(\n",
        "    model=model_bert, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "m_ISnGsrOpmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "OFbngy4mO-Jb",
        "outputId": "93b4e889-7404-441e-f62c-c02e6c9b12ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 04:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=0.6475082600911458, metrics={'train_runtime': 285.2805, 'train_samples_per_second': 10.516, 'train_steps_per_second': 1.314, 'total_flos': 1056031524864000.0, 'train_loss': 0.6475082600911458, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    }
  ]
}